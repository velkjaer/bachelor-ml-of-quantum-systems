
\chaplab{Implementation in Materials Sciences}{implementation}
\thispagestyle{empty}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%   Chapter 4: OUTLINE                              %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\seclab{Crystal structure predictions with machine learning}{discussion}

The search for a universal feature vector suitable for general crystal structure predictions with machine learning continues as the results obtained in this project do not indicate especially important features for all of the targets, $y=\Delta H_{cs\neq r} - \Delta H_{r}$, among our constructed feature vector. The models we were able to create resulted in a far too error prone models, considering the size of the target values. 
The best model for individual prediction was found to be the single-target model as expected in \chapref{ml_in_physics}. This is due to the method allowing different regularization strengths $\lambda$ for the different structure predictions. With the results obtained, it is fair to say that the constructed feature vector, did not fit the bill in order to predict as accurately as required. Partitioning the data better and attempting to predict with the current feature might improve performance, but there are several other approaches waiting to be carried out. For example, one could try to construct feature vectors containing more attributes or try totally different approaches with non-linear algorithms.



\seclab{Future work for machine learning in crystal structure predictions}{future_work}

Further studies on the matter of crystal structure predictions with machine learning could easily be executed as an supplementary research to this project, e.g. by trying some of the proposals above. 

After acknowledging the complexity of the problem with finding a universal descriptor for general crystal structure predictions on this data set, it can be confirmed other approaches are needed. Expert knowledge can be utilized to a greater extent to create proper feature transformations if simple shrinkage methods such as Lasso are to be used or predictions with more complex machine learning tools could be investigated. After creating the three models presented in the project and no common highly important features were found, we decided not to apply forward selection even though it was our initial intention to do so. However, if the data had been partitioned to only octet-compounds like in the studies of \citep{criticalrole_descriptor}, forward selection might be appropriate to construct the low-dimensional descriptors. Generally, the data set could be partitioned in many ways to smaller data sets consisting of materials with more in common, thus simplifying the modelling and likely increasing the performance. \\
If one instead would like to take on the way more complex problem of creating a better predictive machine for general crystal structure predictions, methods like \emph{artificial neural networks} (ANNs) could be utilized. Having suggested that, neural networks have the disadvantage that they perform non-linear mappings. Consequently, they are challenging to interpret. Hence, one could make a better predictive model with less interpretability. In recent years, plural studies have tried to lower the dimensionality and increase the interpretability of ANNs. The process of lowering the dimensionality can be carried out with autoencoders \citep{2016arXiv161107492S} where one tries to construct a lower-dimensional representation of the input signal,
\begin{equation}
    \xxx_i = f_{\mathcal{M}}\qty(\xxx_i,\bsw) + \underbrace{\varepsilon}_{\mathrm{noise}}
\end{equation}
i.e. make a model that can reproduce the signal in a similar way to principal component analysis (PCA). The goal of PCA is also to create a lower dimensional representation by finding the principal components in the data. While PCA is restricted to linear mappings, autoencoders are not. However, due to the this fact, autoencoders tend to make highly non-linear mappings and thus the models are very hard to interpret. This is of course not ideal if one tries to construct interpretable feature vectors. But if a well-predictive relatively low-dimensional model is sought, autoencoders could be applicable.

In terms of the interpretability of artificial neural networks, \citep{Intrator2001} claim that it is possible to make interpretable ANNs if a sufficient amount of regularization is used for modelling along with using their presented methods. Such an approach could be interesting as the feature transformations would be created by the neural network instead of creating them manually. Hence, a study of how one could use ANNs for crystal structure predictions and keep the interpretability intact seems interesting. 


\seclab{Considerations regarding what could have been done differently}{donedifferently}

Initially, it was the authors' intention to use the Open Quantum Materials Database (OQMD), but a lot of problems arose when trying to set up the database. Plenty of time was wasted on the process of setting up the OQMD database, which is a shame considering the extra models we could have created meanwhile. But had the database been set up properly, we might have been able to produce way better results since the number of observations is incomparable to the relatively small data set we have been working with. Hence, we would have had access to a lot more training and test data. Additionally, better data partitioning could have been possible as OQMD now consists of 471857 DFT calculated entries\footnote{The number of current entries was found on \url{oqmd.org}}. Lastly OQMD contains info about more atomic properties per entry than the data set used for modelling in this project and thus, utilizing OQMD allows for a more complex feature vector.
 
 \newpage
 
\chaplab{Conclusion}{conclusion}
 
With high hopes of creating a universal model for crystal structure predictions, using machine learning tools, three models were presented. The single-target model, the multi-target model and the implicitly informed model that all share the property of being rather simple linear regression models with a \Lnorm{1} regularization. The feature vector used for the modelling part of the project was created from initial 34 attribute and a description of which transformations that were are outlined in \secref{main_model}. The final feature vector for both the single-target and multi-target models contains 12330 attributes. For the implicitly informed model, the feature vector has 13656 attributes since the model needed crystal structure dependent features. Hence, the implicitly informed model contains more features and a larger training set. Despite this fact, it does not perform as well as the single target model. Nonetheless, the implicitly informed model confirms that in order to create a universal feature vector for crystal structure predictions, the feature vector will most likely need to contain a lot of complex features created utilizing expert knowledge. Otherwise, a feature vector consisting of way more features than the one created in this study could be constructed using the same methods as presented here, with the purpose of performing a shrinkage method, e.g Lasso in the search for \emph{highly} important universal features. With the 12330/13656-dimensional feature vector created, the best performing model, achieved a RMSE of $\approx 110$ meV and an average absolute deviation from the true targets of $\approx 81$ meV predicting on the relatively complex data set. As the average absolute target value is around $\approx 250$ meV, it is evident that the presented models do not perform well enough to be used for accurate predictions of the differences in heat of formation between Rs and the other crystal structures that were examined. At least not with the full data set. Thus, if the feature vector created in this project is to be used for further modelling, one would need to partition the data differently, e.g., as done by \citep{criticalrole_descriptor}. 
 
 In order to obtain proper results for a general data set such as the one considered throughout this project, other machine learning tools should be considered. For better performance, artificial neural networks could be applicable, even though modelling with neural nets imply a loss of interpretability as they perform non-linear mappings in high-dimensional vector spaces. Considering other kinds of regression types is also relevant, e.g. the ridge regression discussed in \chapref{basicConcepts}.  
 

